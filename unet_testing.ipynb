{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15154b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "# from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as ski\n",
    "from skimage import measure\n",
    "from skimage import util as sk_util\n",
    "from skimage.morphology import remove_small_objects\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b9aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Skin lesion segmentation - great for beginners\n",
    "- URL: https://challenge.isic-archive.com/\n",
    "- Images: ~2000+ dermoscopy images\n",
    "- Task: Melanoma/skin lesion segmentation\n",
    "- Format: RGB images with binary masks\n",
    "\"\"\"\n",
    "\n",
    "# Example loading for ISIC\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        ISIC skin lesion dataset loader.\n",
    "\n",
    "        Expected structure:\n",
    "        root_dir/\n",
    "            ISIC_0000000.jpg\n",
    "            ISIC_0000000_segmentation.png\n",
    "            ...\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        mask_path = os.path.join(self.root_dir,\n",
    "                                img_name.replace('.jpg', '_segmentation.png'))\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(image),\n",
    "                                     mask=np.array(mask))\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return transforms.ToTensor()(image), torch.tensor(np.array(mask) > 0).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec306c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture for semantic segmentation.\n",
    "\n",
    "    U-Net is a convolutional neural network that was developed for biomedical image segmentation.\n",
    "    The architecture consists of:\n",
    "    1. A contracting path (encoder/downsampling) that captures context\n",
    "    2. An expansive path (decoder/upsampling) that enables precise localization\n",
    "    3. Skip connections between encoder and decoder that preserve spatial information\n",
    "\n",
    "    The network has a distinctive U-shaped architecture when visualized, hence the name \"U-Net\".\n",
    "\n",
    "    Key features:\n",
    "    - Symmetric architecture with skip connections\n",
    "    - Combines low-level feature maps with high-level ones\n",
    "    - Works well with limited training data (common in medical imaging)\n",
    "    - Produces dense predictions (output same size as input)\n",
    "\n",
    "    Architecture diagram:\n",
    "\n",
    "    Input (572x572x3) ─┐\n",
    "                      ↓\n",
    "    ┌─────────────────┴─────────────────┐\n",
    "    │          ENCODER PATH             │\n",
    "    │                                   │\n",
    "    │  Conv Block 1 (64 features)       │──── Skip Connection 1 ────┐\n",
    "    │       ↓ MaxPool                   │                           │\n",
    "    │  Conv Block 2 (128 features)      │──── Skip Connection 2 ───┐│\n",
    "    │       ↓ MaxPool                   │                          ││\n",
    "    │  Conv Block 3 (256 features)      │──── Skip Connection 3 ──┐││\n",
    "    │       ↓ MaxPool                   │                         │││\n",
    "    │  Conv Block 4 (512 features)      │──── Skip Connection 4 ─┐│││\n",
    "    │       ↓ MaxPool                   │                        ││││\n",
    "    └───────────────────────────────────┘                        ││││\n",
    "                      ↓                                          ││││\n",
    "    ┌─────────────────┴─────────────────┐                        ││││\n",
    "    │         BOTTLENECK                │                        ││││\n",
    "    │     (1024 features)               │                        ││││\n",
    "    └───────────────────────────────────┘                        ││││\n",
    "                      ↓                                          ││││\n",
    "    ┌─────────────────┴─────────────────┐                        ││││\n",
    "    │          DECODER PATH             │                        ││││\n",
    "    │                                   │                        ││││\n",
    "    │  UpConv + Concat ←───────────────────────────────────────┘│││\n",
    "    │  Conv Block 5 (512 features)      │                         │││\n",
    "    │       ↓                           │                         │││\n",
    "    │  UpConv + Concat ←───────────────────────────────────────┘││\n",
    "    │  Conv Block 6 (256 features)      │                          ││\n",
    "    │       ↓                           │                          ││\n",
    "    │  UpConv + Concat ←───────────────────────────────────────┘│\n",
    "    │  Conv Block 7 (128 features)      │                           │\n",
    "    │       ↓                           │                           │\n",
    "    │  UpConv + Concat ←───────────────────────────────────────┘\n",
    "    │  Conv Block 8 (64 features)       │\n",
    "    │       ↓                           │\n",
    "    │  Final Conv (output_channels)     │\n",
    "    └───────────────────────────────────┘\n",
    "                      ↓\n",
    "               Output (572x572xC)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=2, features=[64, 128, 256, 512]):\n",
    "        \"\"\"\n",
    "        Initialize U-Net model.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "                              (3 for RGB images, 1 for grayscale)\n",
    "            out_channels (int): Number of output channels\n",
    "                               (number of classes for segmentation)\n",
    "                               - 2 for binary segmentation (background + 1 object class)\n",
    "                               - N for multi-class segmentation (background + N-1 object classes)\n",
    "            features (list): Number of features/filters in each encoder layer\n",
    "                            Default: [64, 128, 256, 512]\n",
    "                            Each decoder layer has the same features in reverse order\n",
    "\n",
    "        The network depth is determined by len(features).\n",
    "        Deeper networks can capture more complex patterns but require more memory.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # ModuleList to store encoder and decoder blocks\n",
    "        # Using ModuleList ensures all layers are properly registered with PyTorch\n",
    "        self.ups = nn.ModuleList()    # Decoder/upsampling blocks\n",
    "        self.downs = nn.ModuleList()  # Encoder/downsampling blocks\n",
    "\n",
    "        # MaxPool2d for downsampling between encoder blocks\n",
    "        # kernel_size=2, stride=2 reduces spatial dimensions by half\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #############################################\n",
    "        # ENCODER PATH (LEFT SIDE OF U)\n",
    "        #############################################\n",
    "        # Build encoder blocks progressively\n",
    "        # Each block doubles the number of features while halving spatial dimensions\n",
    "        for feature in features:\n",
    "            # Create double convolution block\n",
    "            # Input: in_channels (3 for first block, then previous feature size)\n",
    "            # Output: feature (64, 128, 256, 512)\n",
    "            self.downs.append(self._block(in_channels, feature))\n",
    "            in_channels = feature  # Update for next iteration\n",
    "\n",
    "        #############################################\n",
    "        # BOTTLENECK (BOTTOM OF U)\n",
    "        #############################################\n",
    "        # Deepest part of the network\n",
    "        # Has the most features but smallest spatial dimensions\n",
    "        # Input: features[-1] (512)\n",
    "        # Output: features[-1]*2 (1024)\n",
    "        # This layer captures the most abstract/high-level features\n",
    "        self.bottleneck = self._block(features[-1], features[-1]*2)\n",
    "\n",
    "        #############################################\n",
    "        # DECODER PATH (RIGHT SIDE OF U)\n",
    "        #############################################\n",
    "        # Build decoder blocks in reverse order\n",
    "        # Each block halves the number of features while doubling spatial dimensions\n",
    "        for feature in reversed(features):\n",
    "            # Transposed convolution for upsampling\n",
    "            # Input: feature*2 (e.g., 1024 for first decoder block)\n",
    "            # Output: feature (e.g., 512)\n",
    "            # kernel_size=2, stride=2 doubles spatial dimensions\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Double convolution block after concatenation with skip connection\n",
    "            # Input: feature*2 (after concatenation with encoder features)\n",
    "            # Output: feature\n",
    "            # The *2 accounts for concatenation with skip connection\n",
    "            self.ups.append(self._block(feature*2, feature))\n",
    "\n",
    "        # Final 1x1 convolution to map to desired number of output channels\n",
    "        # This is essentially a pixel-wise classification layer\n",
    "        # Input: features[0] (64)\n",
    "        # Output: out_channels (number of classes)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def _block(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Create a basic convolutional block used throughout U-Net.\n",
    "\n",
    "        This is the fundamental building block of U-Net, consisting of:\n",
    "        1. 3x3 Convolution (preserves spatial dimensions with padding=1)\n",
    "        2. Batch Normalization (normalizes features, helps training stability)\n",
    "        3. ReLU activation (introduces non-linearity)\n",
    "        4. Another 3x3 Convolution\n",
    "        5. Batch Normalization\n",
    "        6. ReLU activation\n",
    "\n",
    "        This double convolution pattern is used in the original U-Net paper.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: Sequential container of layers\n",
    "\n",
    "        Why this design:\n",
    "        - Two 3x3 convolutions can capture the same receptive field as one 5x5\n",
    "          but with fewer parameters and more non-linearity\n",
    "        - Batch normalization helps with training stability and speed\n",
    "        - ReLU is computationally efficient and helps with gradient flow\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            # First convolution\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),  # inplace=True saves memory\n",
    "\n",
    "            # Second convolution\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the U-Net.\n",
    "\n",
    "        The forward pass follows the U-shaped architecture:\n",
    "        1. Encoder: Progressively downsample while increasing features\n",
    "        2. Bottleneck: Process at the deepest level\n",
    "        3. Decoder: Progressively upsample while decreasing features\n",
    "        4. Skip connections: Concatenate encoder features to decoder\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W)\n",
    "                             B = batch size\n",
    "                             C = channels (e.g., 3 for RGB)\n",
    "                             H = height\n",
    "                             W = width\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output segmentation map of shape (B, num_classes, H, W)\n",
    "\n",
    "        Example dimensions for a 256x256 RGB image:\n",
    "        Input: (B, 3, 256, 256)\n",
    "        After block 1: (B, 64, 256, 256) → After pool: (B, 64, 128, 128)\n",
    "        After block 2: (B, 128, 128, 128) → After pool: (B, 128, 64, 64)\n",
    "        After block 3: (B, 256, 64, 64) → After pool: (B, 256, 32, 32)\n",
    "        After block 4: (B, 512, 32, 32) → After pool: (B, 512, 16, 16)\n",
    "        After bottleneck: (B, 1024, 16, 16)\n",
    "        ... (decoder reverses this process)\n",
    "        Output: (B, num_classes, 256, 256)\n",
    "        \"\"\"\n",
    "        # List to store feature maps from encoder for skip connections\n",
    "        skip_connections = []\n",
    "\n",
    "        #############################################\n",
    "        # ENCODER PATH\n",
    "        #############################################\n",
    "        # Process through each encoder block\n",
    "        for down in self.downs:\n",
    "            # Apply double convolution block\n",
    "            x = down(x)\n",
    "\n",
    "            # Store output for skip connection before pooling\n",
    "            # These features contain important spatial information\n",
    "            # that would be lost after pooling\n",
    "            skip_connections.append(x)\n",
    "\n",
    "            # Downsample using max pooling\n",
    "            # Reduces height and width by half\n",
    "            x = self.pool(x)\n",
    "\n",
    "        #############################################\n",
    "        # BOTTLENECK\n",
    "        #############################################\n",
    "        # Process through the bottleneck (deepest layer)\n",
    "        # At this point, x has smallest spatial dimensions\n",
    "        # but highest number of feature channels\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        #############################################\n",
    "        # DECODER PATH\n",
    "        #############################################\n",
    "        # Reverse the skip connections list to match decoder order\n",
    "        # We concatenate from deepest to shallowest\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # Process through decoder blocks\n",
    "        # Note: self.ups has alternating upconv and conv blocks\n",
    "        # Even indices (0,2,4,...): transposed convolutions\n",
    "        # Odd indices (1,3,5,...): double convolution blocks\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            # 1. Upsample using transposed convolution\n",
    "            x = self.ups[idx](x)\n",
    "\n",
    "            # 2. Get corresponding skip connection from encoder\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            # 3. Handle potential size mismatches\n",
    "            # Due to pooling/upsampling, sizes might not match exactly\n",
    "            # This can happen with odd-sized inputs\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            # 4. Concatenate along channel dimension\n",
    "            # This combines high-resolution features from encoder\n",
    "            # with upsampled features from decoder\n",
    "            # Concatenation happens along channel dim (dim=1)\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "\n",
    "            # 5. Process concatenated features through double conv block\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        #############################################\n",
    "        # FINAL OUTPUT\n",
    "        #############################################\n",
    "        # Apply final 1x1 convolution to get desired number of output channels\n",
    "        # This essentially performs pixel-wise classification\n",
    "        # Output shape: (B, num_classes, H, W)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d385fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import util as sk_util\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, debug=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "\n",
    "        # Load images\n",
    "        image = ski.io.imread(img_path)\n",
    "        mask = ski.io.imread(mask_path)\n",
    "\n",
    "        # Convert to numpy\n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # Debug: print original mask values\n",
    "        if self.debug and idx == 0:\n",
    "            print(f\"\\nDebug Sample {idx}:\")\n",
    "            print(f\"Original mask shape: {mask.shape}\")\n",
    "            print(f\"Original mask dtype: {mask.dtype}\")\n",
    "            print(f\"Original mask unique values: {np.unique(mask)}\")\n",
    "            print(f\"Original mask range: [{mask.min()}, {mask.max()}]\")\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Method 1: Using skimage.util.img_as_float\n",
    "        mask_float = sk_util.img_as_float(mask)  # Converts to [0, 1] float\n",
    "        mask_binary = (mask_float > 0.5).astype(np.int64)  # Threshold at 0.5\n",
    "\n",
    "        # Method 2: Direct conversion\n",
    "        # mask_binary = (mask > 127).astype(np.int64)\n",
    "\n",
    "        # Method 3: Divide and round\n",
    "        # mask_binary = np.round(mask / 255.0).astype(np.int64)\n",
    "\n",
    "        # Method 4: Any non-zero becomes 1\n",
    "        # mask_binary = (mask != 0).astype(np.int64)\n",
    "\n",
    "        if self.debug and idx == 0:\n",
    "            print(f\"After conversion:\")\n",
    "            print(f\"mask_float range: [{mask_float.min()}, {mask_float.max()}]\")\n",
    "            print(f\"mask_binary unique values: {np.unique(mask_binary)}\")\n",
    "\n",
    "        mask_tensor = torch.from_numpy(mask_binary).long()\n",
    "\n",
    "        # Final verification\n",
    "        assert mask_tensor.max() <= 1, f\"Mask has values > 1: {mask_tensor.max()}\"\n",
    "        assert mask_tensor.min() >= 0, f\"Mask has negative values: {mask_tensor.min()}\"\n",
    "\n",
    "        return image, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e84afb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_unet(model, train_loader, val_loader, epochs=50, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train the U-Net model.\n",
    "\n",
    "    Args:\n",
    "        model: U-Net model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        epochs (int): Number of training epochs\n",
    "        lr (float): Learning rate\n",
    "\n",
    "    Returns:\n",
    "        dict: Training history\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de068af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def setup_and_train():\n",
    "    \"\"\"\n",
    "    Set up and train the U-Net model for object detection and classification.\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model = UNet(in_channels=3, out_channels=10)  # Adjust out_channels for your number of classes\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = SegmentationDataset('path/to/train/images', 'path/to/train/masks')\n",
    "    val_dataset = SegmentationDataset('path/to/val/images', 'path/to/val/masks')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # Train model\n",
    "    history = train_unet(model, train_loader, val_loader, epochs=50)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def predict_and_detect_objects(model, image_path):\n",
    "    \"\"\"\n",
    "    Predict segmentation mask and detect individual objects.\n",
    "\n",
    "    Args:\n",
    "        model: Trained U-Net model\n",
    "        image_path (str): Path to input image\n",
    "\n",
    "    Returns:\n",
    "        list: Detected objects with masks and bounding boxes\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "    image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).float() / 255.0\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict segmentation mask\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred_mask = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Post-process to find individual objects\n",
    "    objects = post_process_segmentation(pred_mask)\n",
    "\n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(image_np)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(pred_mask, cmap='tab20')\n",
    "    axes[1].set_title('Segmentation Mask')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    axes[2].imshow(image_np)\n",
    "    for obj in objects:\n",
    "        minr, minc, maxr, maxc = obj['bbox']\n",
    "        rect = plt.Rectangle((minc, minr), maxc-minc, maxr-minr,\n",
    "                           fill=False, edgecolor='red', linewidth=2)\n",
    "        axes[2].add_patch(rect)\n",
    "    axes[2].set_title(f'Detected Objects ({len(objects)})')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return objects\n",
    "\n",
    "# Classification component (can be added after U-Net)\n",
    "class ObjectClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN classifier for classifying detected objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of object classes\n",
    "        \"\"\"\n",
    "        super(ObjectClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0495076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SegmentationDataset(image_dir=r'C:\\Users\\btb51\\Documents\\GitHub\\unet_tests\\data\\images',\n",
    "                              mask_dir=r'C:\\Users\\btb51\\Documents\\GitHub\\unet_tests\\data\\masks',\n",
    "                              debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445f7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = SegmentationDataset(image_dir=r'C:\\Users\\btb51\\Documents\\GitHub\\unet_tests\\data\\val\\images',\n",
    "                               mask_dir=r'C:\\Users\\btb51\\Documents\\GitHub\\unet_tests\\data\\val\\masks', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different conversion methods\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a sample mask\n",
    "mask_path = \"data\\masks\\mask_0000.png\"  # Replace with your mask path\n",
    "mask = ski.io.imread(mask_path)\n",
    "mask_array = np.array(mask)\n",
    "\n",
    "print(f\"Original mask:\")\n",
    "print(f\"  Shape: {mask_array.shape}\")\n",
    "print(f\"  Dtype: {mask_array.dtype}\")\n",
    "print(f\"  Unique values: {np.unique(mask_array)}\")\n",
    "print(f\"  Range: [{mask_array.min()}, {mask_array.max()}]\")\n",
    "\n",
    "# Test different methods\n",
    "methods = {\n",
    "    \"Method 1: skimage.img_as_float\": lambda m: (sk_util.img_as_float(m) > 0.5).astype(np.int64),\n",
    "    \"Method 2: Threshold at 127\": lambda m: (m > 127).astype(np.int64),\n",
    "    \"Method 3: Divide by 255\": lambda m: np.round(m / 255.0).astype(np.int64),\n",
    "    \"Method 4: Non-zero to 1\": lambda m: (m != 0).astype(np.int64),\n",
    "    # \"Method 5: Direct division\": lambda m: (m // 255).astype(np.int64)\n",
    "}\n",
    "\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Plot original\n",
    "# axes[0].imshow(mask_array, cmap='gray')\n",
    "# axes[0].set_title(f\"Original (values: {np.unique(mask_array)})\")\n",
    "\n",
    "# Test each method\n",
    "for i, (name, method) in enumerate(methods.items(), 1):\n",
    "    converted = method(mask_array)\n",
    "    # axes[i].imshow(converted, cmap='gray')\n",
    "    # axes[i].set_title(f\"{name}\\n(values: {np.unique(converted)})\")\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Unique values: {np.unique(converted)}\")\n",
    "    print(f\"  Value counts: {dict(zip(*np.unique(converted, return_counts=True)))}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_segs = post_process_segmentation(dataset[0][1].numpy(), min_size=50)\n",
    "post_segs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(post_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9708efff",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "images, masks = next(iter(train_loader))\n",
    "print(f\"Image batch shape: {images.shape}\")  # [16, 3, 256, 256]\n",
    "print(f\"Mask batch shape: {masks.shape}\")    # [16, 256, 256]\n",
    "print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Unique mask values: {torch.unique(masks)}\")  # Should be [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dca2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_data, batch_size=5, shuffle=True)\n",
    "images, masks = next(iter(val_loader))\n",
    "print(f\"Image batch shape: {images.shape}\")  # [16, 3, 256, 256]\n",
    "print(f\"Mask batch shape: {masks.shape}\")    # [16, 256, 256]\n",
    "print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Unique mask values: {torch.unique(masks)}\")  # Should be [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d744d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force CPU to get better error messages\n",
    "device = torch.device('cpu')\n",
    "model = UNet(in_channels=3, out_channels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Test with a single batch\n",
    "images, masks = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "masks = masks.to(device)\n",
    "\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Masks shape: {masks.shape}\")\n",
    "print(f\"Unique mask values: {torch.unique(masks)}\")\n",
    "print(f\"Mask dtype: {masks.dtype}\")\n",
    "\n",
    "# Try forward pass\n",
    "try:\n",
    "    outputs = model(images)\n",
    "    print(f\"Outputs shape: {outputs.shape}\")\n",
    "    \n",
    "    # Try loss calculation\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs, masks)\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ec56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=3, out_channels=2)  # 2 classes: background, circle\n",
    "trained_model = train_unet(model, train_loader, val_loader, epochs=20)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b401339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09926e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import draw, io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "\n",
    "def generate_circle_image_and_mask(\n",
    "    image_size: Tuple[int, int] = (256, 256),\n",
    "    num_circles: int = 5,\n",
    "    radius_range: Tuple[int, int] = (10, 40),\n",
    "    intensity_range: Tuple[float, float] = (0.3, 0.9),\n",
    "    background_intensity: float = 0.1,\n",
    "    noise_level: float = 0.05,\n",
    "    overlap_allowed: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate synthetic image with circles of varying intensity and corresponding mask.\n",
    "\n",
    "    Args:\n",
    "        image_size: Size of the image (height, width)\n",
    "        num_circles: Number of circles to generate\n",
    "        radius_range: Range of circle radii (min, max)\n",
    "        intensity_range: Range of circle intensities (min, max)\n",
    "        background_intensity: Background pixel intensity\n",
    "        noise_level: Amount of noise to add (0-1)\n",
    "        overlap_allowed: Whether circles can overlap\n",
    "\n",
    "    Returns:\n",
    "        image: Generated image with circles\n",
    "        mask: Binary mask (0=background, 1=circle)\n",
    "    \"\"\"\n",
    "    height, width = image_size\n",
    "\n",
    "    # Initialize image with background\n",
    "    image = np.full((height, width), background_intensity, dtype=np.float32)\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Keep track of existing circles to avoid overlap if needed\n",
    "    existing_circles = []\n",
    "\n",
    "    circles_placed = 0\n",
    "    attempts = 0\n",
    "    max_attempts = num_circles * 100  # Prevent infinite loop\n",
    "\n",
    "    while circles_placed < num_circles and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        # Random circle parameters\n",
    "        radius = random.randint(*radius_range)\n",
    "        center_y = random.randint(radius, height - radius)\n",
    "        center_x = random.randint(radius, width - radius)\n",
    "        intensity = random.uniform(*intensity_range)\n",
    "\n",
    "        # Check for overlap if not allowed\n",
    "        if not overlap_allowed:\n",
    "            overlap = False\n",
    "            for existing_center, existing_radius in existing_circles:\n",
    "                distance = np.sqrt((center_x - existing_center[0])**2 +\n",
    "                                 (center_y - existing_center[1])**2)\n",
    "                if distance < (radius + existing_radius + 5):  # 5 pixel buffer\n",
    "                    overlap = True\n",
    "                    break\n",
    "\n",
    "            if overlap:\n",
    "                continue\n",
    "\n",
    "        # Draw circle\n",
    "        rr, cc = draw.disk((center_y, center_x), radius, shape=image.shape)\n",
    "\n",
    "        # Apply intensity to image\n",
    "        image[rr, cc] = intensity\n",
    "\n",
    "        # Update mask\n",
    "        mask[rr, cc] = 1\n",
    "\n",
    "        # Record circle position\n",
    "        existing_circles.append(((center_x, center_y), radius))\n",
    "        circles_placed += 1\n",
    "\n",
    "    # Add noise to image\n",
    "    if noise_level > 0:\n",
    "        noise = np.random.normal(0, noise_level, image.shape)\n",
    "        image = np.clip(image + noise, 0, 1)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def generate_dataset(\n",
    "    num_images: int = 100,\n",
    "    output_dir: str = \"synthetic_circles\",\n",
    "    image_size: Tuple[int, int] = (256, 256),\n",
    "    num_circles_range: Tuple[int, int] = (3, 8),\n",
    "    **kwargs\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate a dataset of synthetic circle images and masks.\n",
    "\n",
    "    Args:\n",
    "        num_images: Number of images to generate\n",
    "        output_dir: Directory to save images and masks\n",
    "        image_size: Size of each image\n",
    "        num_circles_range: Range for number of circles per image\n",
    "        **kwargs: Additional arguments for generate_circle_image_and_mask\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    image_dir = os.path.join(output_dir, \"images\")\n",
    "    mask_dir = os.path.join(output_dir, \"masks\")\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Random number of circles for this image\n",
    "        num_circles = random.randint(*num_circles_range)\n",
    "\n",
    "        # Generate image and mask\n",
    "        image, mask = generate_circle_image_and_mask(\n",
    "            image_size=image_size,\n",
    "            num_circles=num_circles,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Save image and mask\n",
    "        image_path = os.path.join(image_dir, f\"image_{i:04d}.png\")\n",
    "        mask_path = os.path.join(mask_dir, f\"mask_{i:04d}.png\")\n",
    "\n",
    "        # Convert to uint8 for saving\n",
    "        image_uint8 = (image * 255).astype(np.uint8)\n",
    "        mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "\n",
    "        io.imsave(image_path, image_uint8, check_contrast=False)\n",
    "        io.imsave(mask_path, mask_uint8, check_contrast=False)\n",
    "\n",
    "        print(f\"Generated image {i+1}/{num_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055117b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_multi_class_circles(\n",
    "    image_size: Tuple[int, int] = (256, 256),\n",
    "    circle_types: List[dict] = None,\n",
    "    num_circles_per_type: int = 3,\n",
    "    background_intensity: float = 0.1,\n",
    "    noise_level: float = 0.05\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate synthetic image with different types of circles (multi-class segmentation).\n",
    "\n",
    "    Args:\n",
    "        image_size: Size of the image\n",
    "        circle_types: List of dicts with circle properties (intensity, radius_range, class_id)\n",
    "        num_circles_per_type: Number of circles to generate per type\n",
    "        background_intensity: Background pixel intensity\n",
    "        noise_level: Amount of noise to add\n",
    "\n",
    "    Returns:\n",
    "        image: Generated image\n",
    "        mask: Multi-class mask (0=background, 1,2,3...=different circle types)\n",
    "    \"\"\"\n",
    "    if circle_types is None:\n",
    "        circle_types = [\n",
    "            {\"intensity\": 0.3, \"radius_range\": (5, 15), \"class_id\": 1},   # Small dark circles\n",
    "            {\"intensity\": 0.6, \"radius_range\": (15, 25), \"class_id\": 2},  # Medium gray circles\n",
    "            {\"intensity\": 0.9, \"radius_range\": (25, 40), \"class_id\": 3},  # Large bright circles\n",
    "        ]\n",
    "\n",
    "    height, width = image_size\n",
    "    image = np.full((height, width), background_intensity, dtype=np.float32)\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for circle_type in circle_types:\n",
    "        for _ in range(num_circles_per_type):\n",
    "            radius = random.randint(*circle_type[\"radius_range\"])\n",
    "            center_y = random.randint(radius, height - radius)\n",
    "            center_x = random.randint(radius, width - radius)\n",
    "\n",
    "            rr, cc = draw.disk((center_y, center_x), radius, shape=image.shape)\n",
    "\n",
    "            # Only update if not already occupied (to avoid overlap)\n",
    "            empty_pixels = mask[rr, cc] == 0\n",
    "\n",
    "            image[rr[empty_pixels], cc[empty_pixels]] = circle_type[\"intensity\"]\n",
    "            mask[rr[empty_pixels], cc[empty_pixels]] = circle_type[\"class_id\"]\n",
    "\n",
    "    # Add noise\n",
    "    if noise_level > 0:\n",
    "        noise = np.random.normal(0, noise_level, image.shape)\n",
    "        image = np.clip(image + noise, 0, 1)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def visualize_samples(num_samples: int = 4):\n",
    "    \"\"\"\n",
    "    Generate and visualize sample images with their masks.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Generate image and mask\n",
    "        if i < num_samples // 2:\n",
    "            # Binary segmentation examples\n",
    "            image, mask = generate_circle_image_and_mask(\n",
    "                num_circles=random.randint(3, 7),\n",
    "                overlap_allowed=True if i % 2 == 0 else False\n",
    "            )\n",
    "            title_suffix = \"(Binary - Overlap)\" if i % 2 == 0 else \"(Binary - No Overlap)\"\n",
    "        else:\n",
    "            # Multi-class segmentation examples\n",
    "            image, mask = create_multi_class_circles()\n",
    "            title_suffix = \"(Multi-class)\"\n",
    "\n",
    "        # Original image\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(f'Original Image {title_suffix}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Mask\n",
    "        axes[i, 1].imshow(mask, cmap='tab20' if i >= num_samples // 2 else 'gray')\n",
    "        axes[i, 1].set_title('Segmentation Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # Overlay\n",
    "        overlay = np.stack([image, image, image], axis=-1)\n",
    "        mask_colored = plt.cm.tab20(mask / mask.max())[:, :, :3] if mask.max() > 0 else np.zeros_like(overlay)\n",
    "        overlay = 0.7 * overlay + 0.3 * mask_colored\n",
    "        axes[i, 2].imshow(overlay)\n",
    "        axes[i, 2].set_title('Overlay')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with PyTorch Dataset\n",
    "class SyntheticCircleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for synthetic circle images.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples: int = 1000, transform=None, multi_class: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize synthetic dataset.\n",
    "\n",
    "        Args:\n",
    "            num_samples: Number of samples to generate\n",
    "            transform: Optional transforms\n",
    "            multi_class: If True, generate multi-class segmentation\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.transform = transform\n",
    "        self.multi_class = multi_class\n",
    "\n",
    "        # Pre-generate all samples for consistency\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "\n",
    "        print(f\"Generating {num_samples} synthetic samples...\")\n",
    "        for i in range(num_samples):\n",
    "            if multi_class:\n",
    "                image, mask = create_multi_class_circles()\n",
    "            else:\n",
    "                image, mask = generate_circle_image_and_mask(\n",
    "                    num_circles=random.randint(3, 8)\n",
    "                )\n",
    "\n",
    "            self.images.append(image)\n",
    "            self.masks.append(mask)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{num_samples} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            # Apply augmentations\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Convert to tensors\n",
    "        image = torch.from_numpy(image).unsqueeze(0).float()  # Add channel dimension\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Complete example with training\n",
    "def train_on_synthetic_data():\n",
    "    \"\"\"\n",
    "    Example of training U-Net on synthetic circle data.\n",
    "    \"\"\"\n",
    "    # Create synthetic dataset\n",
    "    train_dataset = SyntheticCircleDataset(num_samples=1000, multi_class=False)\n",
    "    val_dataset = SyntheticCircleDataset(num_samples=200, multi_class=False)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = UNet(in_channels=1, out_channels=2).to(device)  # 1 input channel (grayscale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet_tests3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
